{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "import time\n",
        "import copy\n",
        "import math\n",
        "import numpy as np\n",
        "from numpy.linalg import matrix_power\n",
        "from scipy import sparse\n",
        "import gurobipy as gp\n",
        "from gurobipy import GRB\n",
        "from scipy import sparse\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "###################______________________Loading and Preprocessing of USA Dataset______________________###################\n",
        "\n",
        "node_USA = 221\n",
        "edge_USA = 2166\n",
        "\n",
        "w_matrix_USA = sparse.load_npz('usa_mtx_w_n={}_e={}.npz'.format(node_USA, edge_USA))\n",
        "USA_city_info = pd.read_csv('usa_city_info_n={}_e={}.csv'.format(node_USA, edge_USA))\n",
        "\n",
        "G_USA = nx.from_scipy_sparse_matrix(w_matrix_USA, create_using=nx.DiGraph, edge_attribute = 'weight')\n",
        "\n",
        "alpha = np.array(USA_city_info['pop'], dtype=np.float) / 1000000\n",
        "alpha_USA = {i: alpha[i] for i in range(len(alpha))}\n",
        "theta_USA = copy.deepcopy(alpha_USA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "###################______________________Loading and Preprocessing of Facebook Dataset______________________###################\n",
        "\n",
        "adj_FB = sparse.load_npz('fb_mtx_a_n=600_e=4638.npz')\n",
        "G_f = nx.from_scipy_sparse_matrix(adj_FB, parallel_edges=False, create_using=nx.Graph, edge_attribute = 'weight')\n",
        "\n",
        "alpha_f = {node[1]: 1 for node in enumerate(G_f.nodes)}\n",
        "theta_f = copy.deepcopy(alpha_f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "###################______________________Loading and Preprocessing of Twitter Dataset______________________###################\n",
        "\n",
        "adj_t = sparse.load_npz('twit_mtx_a_n=1000_e=13476.npz')\n",
        "\n",
        "G_t = nx.from_scipy_sparse_matrix(adj_t, create_using=nx.DiGraph, edge_attribute = 'weight')\n",
        "\n",
        "alpha_t = {node[1]: 1 for node in enumerate(G_t.nodes)}\n",
        "theta_t = copy.deepcopy(alpha_t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Egqqey9Yro2S"
      },
      "outputs": [],
      "source": [
        "n1 = 200\n",
        "n2 = 400\n",
        "m2 = 4\n",
        "n3 = 700\n",
        "m3 = 3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_X-c-pX3-oXE"
      },
      "outputs": [],
      "source": [
        "#######################...............Preparing synthetic networks...............#######################\n",
        "\n",
        "G1 = nx.fast_gnp_random_graph(n1, 0.04, seed=20200905)  # RAND\n",
        "G2 = nx.powerlaw_cluster_graph(n2, m2, 0.5, seed=20200904)  # POW-S\n",
        "G3 = nx.powerlaw_cluster_graph(n3, m3, 0.5, seed=20200904)  # POW-L\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Z6hduXoU_9j3",
        "outputId": "da48e3bd-220e-403c-c35f-619ac3de1868"
      },
      "outputs": [],
      "source": [
        "print(len(G1.edges))\n",
        "print(len(G2.edges))\n",
        "print(len(G3.edges))\n",
        "print(len(G_f.edges()))\n",
        "print(len(G_t.edges))\n",
        "print(len(G_USA.edges()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(len(G1.nodes))\n",
        "print(len(G2.nodes))\n",
        "print(len(G3.nodes))\n",
        "print(len(G_f.nodes))\n",
        "print(len(G_t.nodes))\n",
        "print(len(G_USA.nodes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Mn3I7IeEZMl"
      },
      "outputs": [],
      "source": [
        "alpha1 = np.random.randint(1, 10, size=n1)\n",
        "weights1 = np.random.uniform(0.3, 1, size=len(G1.edges()))\n",
        "\n",
        "alpha2 = np.random.randint(1, 10, size=n2)\n",
        "weights2 = np.random.uniform(0.3, 1, size=len(G2.edges()))\n",
        "\n",
        "alpha3 = np.random.randint(1, 10, size=n3)\n",
        "weights3 = np.random.uniform(0.3, 1, size=len(G3.edges()))\n",
        "\n",
        "alpha1 = {node[1]: alpha1[node[1]] for node in enumerate(G1.nodes)}\n",
        "theta1 = copy.deepcopy(alpha1)\n",
        "\n",
        "alpha2 = {node[1]: alpha2[node[1]] for node in enumerate(G2.nodes)}\n",
        "theta2 = copy.deepcopy(alpha2)\n",
        "\n",
        "alpha3 = {node[1]: alpha3[node[1]] for node in enumerate(G3.nodes)}\n",
        "theta3 = copy.deepcopy(alpha3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PB-jWYKKFmSL"
      },
      "outputs": [],
      "source": [
        "nx.set_node_attributes(G1, {node: (theta1[i], alpha1[i]) for i, node in enumerate(G1.nodes)}, 'Attribute')\n",
        "nx.set_node_attributes(G2, {node: (theta2[i], alpha2[i]) for i, node in enumerate(G2.nodes)}, 'Attribute')\n",
        "nx.set_node_attributes(G3, {node: (theta3[i], alpha3[i]) for i, node in enumerate(G3.nodes)}, 'Attribute')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o62QfPOyHvHZ"
      },
      "outputs": [],
      "source": [
        "nx.set_edge_attributes(G1, values={edge: weights1[i] for i, edge in enumerate(G1.edges)}, name='weight')\n",
        "nx.set_edge_attributes(G2, values={edge: weights2[i] for i, edge in enumerate(G2.edges)}, name='weight')\n",
        "nx.set_edge_attributes(G3, values={edge: weights3[i] for i, edge in enumerate(G3.edges)}, name='weight')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVm8CIxlOGWN",
        "outputId": "fdf3d7b1-2672-4a6e-d171-81a452ca0f7c"
      },
      "outputs": [],
      "source": [
        "%pip install gurobipy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gADNVPWKGJQY"
      },
      "outputs": [],
      "source": [
        "from gurobipy import *\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "K = 2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YKVSFylHoWF",
        "outputId": "a9af6eb1-96f5-4f9b-a4b9-280697c6f095"
      },
      "outputs": [],
      "source": [
        "V1 = [node[1] for node in enumerate(G1.nodes)]\n",
        "V2 = [node[1] for node in enumerate(G2.nodes)]\n",
        "V3 = [node[1] for node in enumerate(G3.nodes)]\n",
        "V_f = [node[1] for node in enumerate(G_f.nodes)]\n",
        "V_t = [node[1] for node in enumerate(G_t.nodes)]\n",
        "V_USA = [node[1] for node in enumerate(G_USA.nodes)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "w_1 = nx.get_edge_attributes(G1, \"weight\")\n",
        "w1 = {}\n",
        "for y in w_1:\n",
        "    w1[y[1], y[0]] = w_1[y[0], y[1]]\n",
        "    w1[y[0], y[1]] = w_1[y[0], y[1]]\n",
        "\n",
        "\n",
        "w_2 = nx.get_edge_attributes(G2, \"weight\")\n",
        "w2 = {}\n",
        "for y in w_2:\n",
        "    w2[y[1], y[0]] = w_2[y[0], y[1]]\n",
        "    w2[y[0], y[1]] = w_2[y[0], y[1]]\n",
        "\n",
        "w_3 = nx.get_edge_attributes(G3, \"weight\")\n",
        "w3 = {}\n",
        "for y in w_3:\n",
        "    w3[y[1], y[0]] = w_3[y[0], y[1]]\n",
        "    w3[y[0], y[1]] = w_3[y[0], y[1]]\n",
        "\n",
        "wf = nx.get_edge_attributes(G_f, \"weight\")\n",
        "w_f = {}\n",
        "for y in wf:\n",
        "    w_f[y[1], y[0]] = wf[y[0], y[1]]\n",
        "    w_f[y[0], y[1]] = wf[y[0], y[1]]\n",
        "\n",
        "wt = nx.get_edge_attributes(G_t, \"weight\")\n",
        "w_t = {}\n",
        "for y in wt:\n",
        "    w_t[y[1], y[0]] = wt[y[0], y[1]]\n",
        "    w_t[y[0], y[1]] = 0\n",
        "\n",
        "w_usa = nx.get_edge_attributes(G_USA, \"weight\")\n",
        "w_USA = {}\n",
        "for y in w_usa:\n",
        "    w_USA[y[1], y[0]] = w_usa[y[0], y[1]]\n",
        "    w_USA[y[0], y[1]] = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def milp_model(G, V, theta, w, alpha):\n",
        "    model = gp.Model(\"LP_Model\")\n",
        "\n",
        "    #R = 0.5*sum(theta.values())\n",
        "    r_keys = []\n",
        "    k_nbrs = {}\n",
        "    n_nbrs = {}\n",
        "    \n",
        "    for u in V:\n",
        "        n_nbrs[u] = []\n",
        "        for v in nx.single_source_shortest_path_length(G, u, cutoff=1).keys():\n",
        "            if(u!=v):\n",
        "                n_nbrs[u].append(v)\n",
        "        sorted(n_nbrs[u])\n",
        "    \n",
        "   \n",
        "    for u in V:\n",
        "        r_keys.append(u)\n",
        "        k_nbrs[u] = []\n",
        "        for v in nx.single_source_shortest_path_length(G, u, cutoff=K).keys():\n",
        "            k_nbrs[u].append(v)\n",
        "    \n",
        "    x_keys = []\n",
        "    t_keys = []\n",
        "\n",
        "    for i, items in k_nbrs.items():\n",
        "        arr_i = np.ones((len(items), 1)) * i\n",
        "        arr_v = np.array(items)[:, np.newaxis]\n",
        "        arr_tmp = np.concatenate((arr_i, arr_v), axis=1)\n",
        "        x_keys += list(map(tuple, arr_tmp))\n",
        "    \n",
        "        i_keys = []\n",
        "        i_dict = []\n",
        "        for j in items:\n",
        "            if len(n_nbrs[j]) == 0:\n",
        "                continue\n",
        "            j_nbrs = np.array(n_nbrs[j])[:, np.newaxis]\n",
        "            arr_i = np.ones((len(n_nbrs[j]), 1)) * i\n",
        "            arr_j = np.ones((len(n_nbrs[j]), 1)) * j\n",
        "            arr_tmp = np.concatenate((arr_i, j_nbrs, arr_j), axis=1)\n",
        "            arr_dict = np.concatenate((j_nbrs, arr_j), axis=1)\n",
        "            i_keys += list(map(tuple, arr_tmp))\n",
        "            i_dict += list(map(tuple, arr_dict))\n",
        "                \n",
        "        t_keys += i_keys\n",
        "\n",
        "    obj = model.addVar(vtype=GRB.CONTINUOUS, name=\"obj\")\n",
        "    r = model.addVars(r_keys, vtype=GRB.CONTINUOUS, name='r')\n",
        "    b = model.addVars(t_keys, vtype=GRB.CONTINUOUS, name='b')\n",
        "    model.setParam('OutputFlag', 0)\n",
        "\n",
        "    #model.addConstr(r.sum() <= resource, name='total resource sum')\n",
        "    model.addConstrs((b[(i, j, k)] <= w[j, k] * r[j] for (i, j, k) in t_keys), name='borrowing limitations')\n",
        "    model.addConstrs((gp.quicksum((1 - 1) * alpha[k] for k in k_nbrs[i]) <= obj\n",
        "                  for i in range(len(V))), name='minimize maximum')\n",
        "    for ra in range(len(V)):\n",
        "        model.addConstrs((r[k] - gp.quicksum(b.select(ra, k, '*')) + gp.quicksum(b.select(ra, '*', k))\n",
        "                      >= theta[k] * 1 for k in k_nbrs[ra]),\n",
        "                     name='reallocation results for node {}'.format(ra))\n",
        "        model.addConstrs((gp.quicksum(b.select(ra, k, '*')) <= r[k] for k in range(len(V))),\n",
        "                     name='total out-borrowings')\n",
        "\n",
        "    model.setObjective(obj, GRB.MINIMIZE)\n",
        "\n",
        "    model.setParam('OutputFlag', 0)\n",
        "    lb_sum = sum(theta.values())\n",
        "    resource_low = 0\n",
        "    resource_high = math.ceil(lb_sum)\n",
        "\n",
        "    flag = 0\n",
        "    min_value = sum(alpha.values())+1\n",
        "\n",
        "    while resource_low < resource_high:\n",
        "        resource = (resource_low + resource_high) / 2\n",
        "        if(flag == 0):\n",
        "            flag = 1\n",
        "            model.addConstr(r.sum() <= resource, name='total resource sum')\n",
        "            model.update()\n",
        "            model.optimize()\n",
        "            time_LP = model.Runtime\n",
        "            model.setParam('TimeLimit', 3*time_LP)\n",
        "            model.update()\n",
        "        else:\n",
        "            obj_constrain = model.getConstrByName('total resource sum')\n",
        "            model.remove(obj_constrain)\n",
        "            model.addConstr(r.sum() <= resource, name='total resource sum')\n",
        "            model.update()\n",
        "            model.optimize()\n",
        "    \n",
        "        if(model.status == 3 or model.status == 9):\n",
        "            resource_low = resource\n",
        "            if resource_high - resource < 1:\n",
        "                break\n",
        "            continue\n",
        "        obj_curr = model.getObjective().getValue()\n",
        "\n",
        "        if obj_curr == 0:\n",
        "            if resource_high - resource < 1:\n",
        "                break\n",
        "            resource_high = resource\n",
        "        else:\n",
        "            if obj_curr < min_value:\n",
        "                min_value = obj_curr\n",
        "            resource_low = resource\n",
        "\n",
        "    resource = resource + 1\n",
        "    \n",
        "    return resource\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "R_sol_1 = milp_model(G1, V1, theta1, w1, alpha1)\n",
        "R_sol_2 = milp_model(G2, V2, theta2, w2, alpha2)\n",
        "R_sol_3 = milp_model(G3, V3, theta3, w3, alpha3)\n",
        "R_sol_f = milp_model(G_f, V_f, theta_f, w_f, alpha_f)\n",
        "R_sol_t = milp_model(G_t, V_t, theta_t, w_t, alpha_t)\n",
        "R_sol_USA = milp_model(G_USA, V_USA, theta_USA, w_USA, alpha_USA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(R_sol_1, R_sol_2, R_sol_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "w_1 = nx.get_edge_attributes(G1, \"weight\")\n",
        "w1_zero = {}\n",
        "for y in w_1:\n",
        "    w1_zero[y[1], y[0]] = 0\n",
        "    w1_zero[y[0], y[1]] = 0\n",
        "\n",
        "\n",
        "w_2 = nx.get_edge_attributes(G2, \"weight\")\n",
        "w2_zero = {}\n",
        "for y in w_2:\n",
        "    w2_zero[y[1], y[0]] = 0\n",
        "    w2_zero[y[0], y[1]] = 0\n",
        "\n",
        "w_3 = nx.get_edge_attributes(G3, \"weight\")\n",
        "w3_zero = {}\n",
        "for y in w_3:\n",
        "    w3_zero[y[1], y[0]] = 0\n",
        "    w3_zero[y[0], y[1]] = 0\n",
        "\n",
        "wf = nx.get_edge_attributes(G_f, \"weight\")\n",
        "w_f_zero = {}\n",
        "for y in wf:\n",
        "    w_f_zero[y[1], y[0]] = 0\n",
        "    w_f_zero[y[0], y[1]] = 0\n",
        "\n",
        "wt = nx.get_edge_attributes(G_t, \"weight\")\n",
        "w_t_zero = {}\n",
        "for y in wt:\n",
        "    w_t_zero[y[0], y[1]] = 0\n",
        "    w_t_zero[y[1], y[0]] = 0\n",
        "\n",
        "w_usa = nx.get_edge_attributes(G_USA, \"weight\")\n",
        "w_USA_zero = {}\n",
        "for y in w_usa:\n",
        "    w_USA_zero[y[0], y[1]] = 0\n",
        "    w_USA_zero[y[1], y[0]] = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "R_sol_wo_realloc_1 = milp_model(G1, V1, theta1, w1_zero, alpha1)\n",
        "R_sol_wo_realloc_2 = milp_model(G2, V2, theta2, w2_zero, alpha2)\n",
        "R_sol_wo_realloc_3 = milp_model(G3, V3, theta3, w3_zero, alpha3)\n",
        "R_sol_wo_realloc_f = milp_model(G_f, V_f, theta_f, w_f_zero, alpha_f)\n",
        "R_sol_wo_realloc_t = milp_model(G_t, V_t, theta_t, w_t_zero, alpha_t)\n",
        "R_sol_wo_realloc_USA = milp_model(G_USA, V_USA, theta_USA, w_USA_zero, alpha_USA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(R_sol_wo_realloc_1, R_sol_wo_realloc_2, R_sol_wo_realloc_3, R_sol_wo_realloc_USA, R_sol_wo_realloc_f, R_sol_wo_realloc_t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Greedy(G, V, theta, alpha):\n",
        "    R = 0.5*sum(theta.values())\n",
        "    max_loss = 0\n",
        "    #alpha_x = {v:alpha[v] for v in nx.single_source_shortest_path_length(G, u, cutoff=K).keys()}\n",
        "    sorted_alpha = dict(sorted(alpha.items(), key=lambda x: x[1], reverse=True))\n",
        "    allocation = {key: 0 for key in sorted_alpha.keys()}\n",
        "    allocated = 0\n",
        "\n",
        "    for key in sorted_alpha.keys():\n",
        "        if((R-allocated) >= theta[key]):\n",
        "            allocation[key] = theta[key]\n",
        "            allocated += allocation[key]\n",
        "        else:\n",
        "            allocation[key] = R-allocated\n",
        "            break\n",
        "\n",
        "    for u in V:\n",
        "\n",
        "        alpha_x = {v: alpha[v] for v in nx.single_source_shortest_path_length(G, u, cutoff=K).keys()}\n",
        "        #sorted_alpha = dict(sorted(alpha_x.items(), key=lambda x:x[1], reverse=True))\n",
        "        #allocation={key:0 for key in sorted_alpha.keys()}\n",
        "        # allocated=0\n",
        "\n",
        "        loss = 0\n",
        "        for key in alpha_x.keys():\n",
        "            if(allocation[key] < theta[key]):\n",
        "                loss += alpha_x[key]\n",
        "\n",
        "        if(loss > max_loss):\n",
        "            max_loss = loss\n",
        "\n",
        "    return max_loss, allocation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss_greedy1, allocation1 = Greedy(G1, V1, theta1, alpha1)\n",
        "loss_greedy2, allocation2 = Greedy(G2, V2, theta2, alpha2)\n",
        "loss_greedy3, allocation3 = Greedy(G3, V3, theta3, alpha3)\n",
        "loss_greedy_USA, allocation_USA = Greedy(G_USA, V_USA, theta_USA, alpha_USA)\n",
        "loss_greedy_f, allocation_f = Greedy(G_f, V_f, theta_f, alpha_f)\n",
        "loss_greedy_t, allocation_t = Greedy(G_t, V_t, theta_t, alpha_t)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(loss_greedy1, loss_greedy2, loss_greedy3, loss_greedy_USA , loss_greedy_f, loss_greedy_t)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Reallocation(G, V):\n",
        "\n",
        "    reallocation = {}\n",
        "    \n",
        "    for u in V:\n",
        "        for v in nx.single_source_shortest_path_length(G, u, cutoff=K).keys():\n",
        "            for z in nx.single_source_shortest_path_length(G, v, cutoff=1).keys():\n",
        "                if(z != v):\n",
        "                    reallocation[(u,v,z)] = 0\n",
        "                    reallocation[(u,z,v)] = 0\n",
        "    \n",
        "    return reallocation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reallocation1 = Reallocation(G1, V1)\n",
        "reallocation2 = Reallocation(G2, V2)\n",
        "reallocation3 = Reallocation(G3, V3)\n",
        "reallocation_f = Reallocation(G_f, V_f)\n",
        "reallocation_t = Reallocation(G_t, V_t)\n",
        "reallocation_USA = Reallocation(G_USA, V_USA)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def reallocation_from_neighbour(G, u, target, reallocation):\n",
        "    totalReallocation = 0.0\n",
        "    for vertex in nx.single_source_shortest_path_length(G, target, cutoff=1).keys():\n",
        "        if(vertex!=target):\n",
        "            totalReallocation += reallocation[(u, vertex, target)]\n",
        "    return totalReallocation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Greedy_R(G, V, theta, w, alpha, Allocation, reallocation):\n",
        "    max_loss = 0\n",
        "    defending_power = {(u, v): copy.deepcopy(Allocation[v]) for u in V for v in nx.single_source_shortest_path_length(G, u, cutoff=K).keys()}\n",
        "    for u in V:\n",
        "        allocation = copy.deepcopy(Allocation)\n",
        "        alpha_x = {v: alpha[v] for v in nx.single_source_shortest_path_length(G, u, cutoff=K).keys() if(Allocation[v]<theta[v])}\n",
        "        sorted_alpha_x = dict(sorted(alpha_x.items(), key=lambda x: x[1], reverse=True))\n",
        "        for key in sorted_alpha_x.keys():\n",
        "            for neighbour in nx.single_source_shortest_path_length(G, key, cutoff=1).keys():\n",
        "                if neighbour==key:\n",
        "                    continue\n",
        "                if neighbour not in nx.single_source_shortest_path_length(G, u, cutoff=K).keys():\n",
        "                    curr_demand = theta[key] - Allocation[key] - reallocation_from_neighbour(G, u, key, reallocation)\n",
        "                    if(curr_demand > 0):\n",
        "                        if(w[neighbour, key]*allocation[neighbour] <= curr_demand):\n",
        "                            reallocation[(u, neighbour, key)] += w[neighbour, key]*allocation[neighbour]\n",
        "                            allocation[neighbour] = 0\n",
        "                        else:\n",
        "                            reallocation[(u, neighbour, key)] += curr_demand\n",
        "                            allocation[neighbour] -= curr_demand\n",
        "                    else:\n",
        "                        break\n",
        "    # print(reallocation)\n",
        "\n",
        "    for u in V:\n",
        "        for vertex in nx.single_source_shortest_path_length(G, u, cutoff=K).keys():\n",
        "            for neighbour in nx.single_source_shortest_path_length(G, vertex, cutoff=1).keys():\n",
        "                if(neighbour!=vertex):\n",
        "                    if neighbour in nx.single_source_shortest_path_length(G, u, cutoff=K).keys():\n",
        "                        defending_power[(u, vertex)] += (reallocation[(u, neighbour,vertex)] - reallocation[(u, vertex, neighbour)])\n",
        "                    else:\n",
        "                        defending_power[(u, vertex)] += reallocation[(u, neighbour, vertex)]\n",
        "\n",
        "    for u in V:\n",
        "        loss = 0\n",
        "        for v in nx.single_source_shortest_path_length(G, u, cutoff=K).keys():\n",
        "            if(defending_power[(u, v)] < theta[v]):\n",
        "                loss += alpha[v]\n",
        "        if(loss > max_loss):\n",
        "            max_loss = loss\n",
        "\n",
        "    return max_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss_greedy_r1 = Greedy_R(G1, V1, theta1, w1, alpha1, allocation1, reallocation1)\n",
        "loss_greedy_r2 = Greedy_R(G2, V2, theta2, w2, alpha2, allocation2, reallocation2)\n",
        "loss_greedy_r3 = Greedy_R(G3, V3, theta3, w3, alpha3, allocation3, reallocation3)\n",
        "loss_greedy_r_USA = Greedy_R(G_USA, V_USA, theta_USA, w_USA, alpha_USA, allocation_USA, reallocation_USA)\n",
        "loss_greedy_r_f = Greedy_R(G_f, V_f, theta_f, w_f, alpha_f, allocation_f, reallocation_f)\n",
        "loss_greedy_r_t = Greedy_R(G_t, V_t, theta_t, w_t, alpha_t, allocation_t, reallocation_t)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(loss_greedy_r1, loss_greedy_r2, loss_greedy_r3, loss_greedy_r_USA, loss_greedy_r_f, loss_greedy_r_t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#######################...............Defining the LP Model for BA_epsilon algorithm...............#######################\n",
        "\n",
        "def BA_epsilon(G, V, theta, w, alpha, epsilon):\n",
        "    \n",
        "    model = gp.Model(\"LP_Model\")\n",
        "\n",
        "    R = epsilon*0.5*sum(theta.values())\n",
        "    r_keys = []\n",
        "    k_nbrs = {}\n",
        "    n_nbrs = {}\n",
        "    \n",
        "    for u in V:\n",
        "        n_nbrs[u] = []\n",
        "        for v in nx.single_source_shortest_path_length(G, u, cutoff=1).keys():\n",
        "            if(u!=v):\n",
        "                n_nbrs[u].append(v)\n",
        "        sorted(n_nbrs[u])\n",
        "    \n",
        "   \n",
        "    for u in V:\n",
        "        r_keys.append(u)\n",
        "        k_nbrs[u] = []\n",
        "        for v in nx.single_source_shortest_path_length(G, u, cutoff=K).keys():\n",
        "            k_nbrs[u].append(v)\n",
        "    \n",
        "    x_keys = []\n",
        "    t_keys = []\n",
        "\n",
        "    for i, items in k_nbrs.items():\n",
        "        arr_i = np.ones((len(items), 1)) * i\n",
        "        arr_v = np.array(items)[:, np.newaxis]\n",
        "        arr_tmp = np.concatenate((arr_i, arr_v), axis=1)\n",
        "        x_keys += list(map(tuple, arr_tmp))\n",
        "    \n",
        "        i_keys = []\n",
        "        i_dict = []\n",
        "        for j in items:\n",
        "            if len(n_nbrs[j]) == 0:\n",
        "                continue\n",
        "            j_nbrs = np.array(n_nbrs[j])[:, np.newaxis]\n",
        "            arr_i = np.ones((len(n_nbrs[j]), 1)) * i\n",
        "            arr_j = np.ones((len(n_nbrs[j]), 1)) * j\n",
        "            arr_tmp = np.concatenate((arr_i, j_nbrs, arr_j), axis=1)\n",
        "            arr_dict = np.concatenate((j_nbrs, arr_j), axis=1)\n",
        "            i_keys += list(map(tuple, arr_tmp))\n",
        "            i_dict += list(map(tuple, arr_dict))\n",
        "                \n",
        "        t_keys += i_keys\n",
        "\n",
        "    obj = model.addVar(vtype=GRB.CONTINUOUS, name=\"obj\")\n",
        "    r = model.addVars(r_keys, vtype=GRB.CONTINUOUS, name='r')\n",
        "    x = model.addVars(x_keys, lb=0, ub=1, vtype=GRB.CONTINUOUS, name='x')\n",
        "    b = model.addVars(t_keys, vtype=GRB.CONTINUOUS, name='b')\n",
        "    model.setParam('OutputFlag', 0)\n",
        "\n",
        "    model.addConstr(r.sum() <= resource, name='total resource sum')\n",
        "    model.addConstrs((b[(i, j, k)] <= w[j, k] * r[j] for (i, j, k) in t_keys), name='borrowing limitations')\n",
        "    model.addConstrs((gp.quicksum((1 - 1) * alpha[k] for k in k_nbrs[i]) <= obj\n",
        "                  for i in range(len(V))), name='minimize maximum')\n",
        "    for ra in range(len(V)):\n",
        "        model.addConstrs((r[k] - gp.quicksum(b.select(ra, k, '*')) + gp.quicksum(b.select(ra, '*', k))\n",
        "                      >= theta[k] * 1 for k in k_nbrs[ra]),\n",
        "                     name='reallocation results for node {}'.format(ra))\n",
        "        model.addConstrs((gp.quicksum(b.select(ra, k, '*')) <= r[k] for k in range(len(V))),\n",
        "                     name='total out-borrowings')\n",
        "\n",
        "    model.setObjective(obj, GRB.MINIMIZE)\n",
        "\n",
        "    model.setParam('OutputFlag', 0)\n",
        "    model.optimize()\n",
        "\n",
        "    \n",
        "    if(model.status != GRB.OPTIMAL):\n",
        "        return None\n",
        "    else:\n",
        "        solution_x = {}\n",
        "        sol_x = {}\n",
        "        for u in V:\n",
        "            for v in nx.single_source_shortest_path_length(G, u, cutoff=K).keys():\n",
        "                sol_x[u, v] = x[u, v].X\n",
        "                if(x[u, v].X > epsilon):\n",
        "                    solution_x[u, v] = 1\n",
        "                else:\n",
        "                    solution_x[u, v] = 0\n",
        "\n",
        "        max_loss = 0\n",
        "        for u in V:\n",
        "            loss = 0\n",
        "            for v in nx.single_source_shortest_path_length(G, u, cutoff=K).keys():\n",
        "                loss += (1-solution_x[u, v])*alpha[v]\n",
        "            if(loss > max_loss):\n",
        "                max_loss = loss\n",
        "\n",
        "        return max_loss, sol_x, model.getObjective().getValue()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#######################...............Defining the LP Model for BA_tau algorithm...............#######################\n",
        "\n",
        "def BA_tau(G, V, theta, w, alpha, solu_x):\n",
        "    \n",
        "    model = gp.Model(\"LP_Model\")\n",
        "\n",
        "    R = 0.5*sum(theta.values())\n",
        "    sol_x = copy.deepcopy(solu_x)\n",
        "    r_keys = []\n",
        "    k_nbrs = {}\n",
        "    n_nbrs = {}\n",
        "    \n",
        "    for u in V:\n",
        "        n_nbrs[u] = []\n",
        "        for v in nx.single_source_shortest_path_length(G, u, cutoff=1).keys():\n",
        "            if(u!=v):\n",
        "                n_nbrs[u].append(v)\n",
        "        sorted(n_nbrs[u])\n",
        "    \n",
        "   \n",
        "    for u in V:\n",
        "        r_keys.append(u)\n",
        "        k_nbrs[u] = []\n",
        "        for v in nx.single_source_shortest_path_length(G, u, cutoff=K).keys():\n",
        "            k_nbrs[u].append(v)\n",
        "    \n",
        "    x_keys = []\n",
        "    t_keys = []\n",
        "\n",
        "    for i, items in k_nbrs.items():\n",
        "        arr_i = np.ones((len(items), 1)) * i\n",
        "        arr_v = np.array(items)[:, np.newaxis]\n",
        "        arr_tmp = np.concatenate((arr_i, arr_v), axis=1)\n",
        "        x_keys += list(map(tuple, arr_tmp))\n",
        "    \n",
        "        i_keys = []\n",
        "        i_dict = []\n",
        "        for j in items:\n",
        "            if len(n_nbrs[j]) == 0:\n",
        "                continue\n",
        "            j_nbrs = np.array(n_nbrs[j])[:, np.newaxis]\n",
        "            arr_i = np.ones((len(n_nbrs[j]), 1)) * i\n",
        "            arr_j = np.ones((len(n_nbrs[j]), 1)) * j\n",
        "            arr_tmp = np.concatenate((arr_i, j_nbrs, arr_j), axis=1)\n",
        "            arr_dict = np.concatenate((j_nbrs, arr_j), axis=1)\n",
        "            i_keys += list(map(tuple, arr_tmp))\n",
        "            i_dict += list(map(tuple, arr_dict))\n",
        "                \n",
        "        t_keys += i_keys\n",
        "\n",
        "    obj = model.addVar(vtype=GRB.CONTINUOUS, name=\"obj\")\n",
        "    r = model.addVars(r_keys, vtype=GRB.CONTINUOUS, name='r')\n",
        "    x = model.addVars(x_keys, lb=0, ub=1, vtype=GRB.CONTINUOUS, name='x')\n",
        "    b = model.addVars(t_keys, vtype=GRB.CONTINUOUS, name='b')\n",
        "    model.setParam('OutputFlag', 0)\n",
        "\n",
        "    model.addConstr(r.sum() <= resource, name='total resource sum')\n",
        "    model.addConstrs((b[(i, j, k)] <= w[j, k] * r[j] for (i, j, k) in t_keys), name='borrowing limitations')\n",
        "    model.addConstrs((gp.quicksum((1 - 1) * alpha[k] for k in k_nbrs[i]) <= obj\n",
        "                  for i in range(len(V))), name='minimize maximum')\n",
        "    for ra in range(len(V)):\n",
        "        model.addConstrs((r[k] - gp.quicksum(b.select(ra, k, '*')) + gp.quicksum(b.select(ra, '*', k))\n",
        "                      >= theta[k] * 1 for k in k_nbrs[ra]),\n",
        "                     name='reallocation results for node {}'.format(ra))\n",
        "        model.addConstrs((gp.quicksum(b.select(ra, k, '*')) <= r[k] for k in range(len(V))),\n",
        "                     name='total out-borrowings')\n",
        "\n",
        "    model.setObjective(obj, GRB.MINIMIZE)\n",
        "\n",
        "    model.setParam('OutputFlag', 0)\n",
        "    model.optimize()\n",
        "\n",
        "    flag=0\n",
        "    for key in sol_x.keys():\n",
        "        if sol_x[key]<0 and sol_x[key]<1:\n",
        "            flag=2\n",
        "            break\n",
        "    if flag==2:\n",
        "        tau = 0\n",
        "        tau_low = 0\n",
        "        tau_high = 0\n",
        "    else:\n",
        "        tau_low = 0\n",
        "        tau_high = max(sol_x.values())\n",
        "        flag=3\n",
        "    \n",
        "    mark=0\n",
        "    while(tau_high - tau_low >= 0.05):\n",
        "        tau = (tau_high + tau_low)/2\n",
        "        sol_x = copy.deepcopy(solu_x)\n",
        "        for key in sol_x.keys():\n",
        "            if sol_x[key] > tau:\n",
        "                sol_x[key] = 1\n",
        "            else:\n",
        "                sol_x[key] = 0\n",
        "        for u in V:\n",
        "            for v in k_nbrs[u]:\n",
        "                if sol_x[u, v] == 1:\n",
        "                    model.addConstr(x[(u,v)] >= sol_x[u, v], name='x_fixed for node {}_{}'.format(u,v))\n",
        "                else:\n",
        "                    model.addConstr(x[(u,v)] <= sol_x[u, v], name='x_fixed for node {}_{}'.format(u,v))\n",
        "        \n",
        "        model.update()\n",
        "        model.optimize()\n",
        "        if model.status != GRB.OPTIMAL:  # Infeasible\n",
        "            tau_low = tau\n",
        "            flag = 0\n",
        "        else:\n",
        "            tau_high = tau\n",
        "            flag = 1\n",
        "            mark = 1  # exist at least one feasible tau\n",
        "    \n",
        "    if mark==0:\n",
        "        flag=3\n",
        "    \n",
        "    \n",
        "    if flag==3:    # if tau_high is too low\n",
        "        tau = tau_high\n",
        "        sol_x = copy.deepcopy(solu_x)\n",
        "        for key in sol_x.keys():\n",
        "            if sol_x[key] > tau:\n",
        "                sol_x[key] = 1\n",
        "            else:\n",
        "                sol_x[key] = 0\n",
        "        for u in V:\n",
        "            for v in k_nbrs[u]:\n",
        "                obj_constrain = model.getConstrByName('x_fixed for node {}_{}'.format(u,v))\n",
        "                model.remove(obj_constrain)\n",
        "        for u in V:\n",
        "            for v in k_nbrs[u]:\n",
        "                if sol_x[u, v] == 1:\n",
        "                    model.addConstr(x[(u,v)] >= sol_x[u, v], name='x_fixed for node {}_{}'.format(u,v))\n",
        "                else:\n",
        "                    model.addConstr(x[(u,v)] <= sol_x[u, v], name='x_fixed for node {}_{}'.format(u,v))\n",
        "        model.update()\n",
        "        model.optimize()\n",
        "    \n",
        "    if flag == 0:\n",
        "        tau = tau_high\n",
        "        sol_x = copy.deepcopy(solu_x)\n",
        "        for key in sol_x.keys():\n",
        "            if sol_x[key] > tau:\n",
        "                sol_x[key] = 1\n",
        "            else:\n",
        "                sol_x[key] = 0\n",
        "    \n",
        "    max_loss = 0\n",
        "    for u in V:\n",
        "        loss = 0\n",
        "        for v in k_nbrs[u]:\n",
        "            loss += (1-sol_x[u, v])*alpha[v]\n",
        "        if(loss > max_loss):\n",
        "            max_loss = loss\n",
        "\n",
        "    return tau, max_loss, \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def BA(G, V, theta, w, alpha):\n",
        "    loss_epsilon = {}\n",
        "    loss_epsilon_tau = {}\n",
        "    defend_result_LP = {}\n",
        "    epsilons = [0.9,1]\n",
        "    for epsilon in epsilons:\n",
        "        loss_epsilon[epsilon], solu_x, defend_result_LP[epsilon] = BA_epsilon(G, V, theta, w, alpha, epsilon)\n",
        "        tau, loss_epsilon_tau[epsilon, tau] = BA_tau(G, V, theta, w, alpha, solu_x)\n",
        "    return loss_epsilon, loss_epsilon_tau, defend_result_LP\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "BA_epsilon_loss1, BA_tau_loss1, exact_defend_result1 = BA(G1, V1, theta1, w1, alpha1)\n",
        "BA_epsilon_loss2, BA_tau_loss2, exact_defend_result2 = BA(G2, V2, theta2, w2, alpha2)\n",
        "BA_epsilon_loss3, BA_tau_loss3, exact_defend_result3 = BA(G3, V3, theta3, w3, alpha3)\n",
        "BA_epsilon_loss_USA, BA_tau_loss_USA, exact_defend_result_USA = BA(G_USA, V_USA, theta_USA, w_USA, alpha_USA)\n",
        "BA_epsilon_loss_f, BA_tau_loss_f, exact_defend_result_f = BA(G_f, V_f, theta_f, w_f, alpha_f)\n",
        "BA_epsilon_loss_t, BA_tau_loss_t, exact_defend_result_t = BA(G_t, V_t, theta_t, w_t, alpha_t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(min(BA_epsilon_loss1.values()), min(BA_tau_loss1.values()), min(exact_defend_result1.values()))\n",
        "print(min(BA_epsilon_loss2.values()), min(BA_tau_loss2.values()), min(exact_defend_result2.values()))\n",
        "print(min(BA_epsilon_loss3.values()), min(BA_tau_loss3.values()), min(exact_defend_result3.values()))\n",
        "print(min(BA_epsilon_loss_USA.values()), min(BA_tau_loss_USA.values()), min(exact_defend_result_USA.values()))\n",
        "print(min(BA_epsilon_loss_f.values()), min(BA_tau_loss_f.values()), min(exact_defend_result_f.values()))\n",
        "print(min(BA_epsilon_loss_t.values()), min(BA_tau_loss_t.values()), min(exact_defend_result_t.values()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(sum(theta1.values()))\n",
        "print(sum(theta2.values()))\n",
        "print(sum(theta3.values()))\n",
        "print(sum(theta_f.values()))\n",
        "print(sum(theta_t.values()))\n",
        "print(sum(theta_USA.values()))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
